# Exploring the Reasoning Languages of the DeepSeek R1 (671B) Model

This repository is intended to help researchers and developers understand which languages are used by the DeepSeek R1 (671B) model when generating Chain-of-Thought (CoT) reasoning samples. The goal is to provide insights into how the model responds to different input languages and what reasoning languages it produces.

These findings are particularly useful for those looking to use DeepSeek R1 to generate CoT samples for fine-tuning other scalable language models (SLMs), such as LLaMA or Qwen, in order to adapt them to specific business domain knowledge—like data governance policy, healthcare, or legal frameworks.

By exploring the TABLE below, you can gain a better understanding of the reasoning language dynamics and use this knowledge to enhance your fine-tuning process for specialized tasks.

## Languages mapping of reasoning context generated by the DeepSeek R1 (671B) Model

The /think column represents the reasoning context generated by the model. Based on our observations, most of the reasoning contexts are produced in English. Among the input languages, Korean and Simplified/Traditional Chinese yield reasoning contexts that closely align with the input, while Japanese poses the greatest challenge.

If the input question is in Japanese, you may need an additional workflow to translate the Chain-of-Thought (CoT) samples generated in Simplified Chinese into Japanese to ensure compatibility and accuracy.


| Prefix | Question            | Answer            | `</think>`         |
|--------|---------------------|-------------------|--------------------|
| ar     | Arabic              | Arabic            | English            |
| be     | Bengali             | Bengali           | English            |
| en     | English             | English           | English            |
| fr     | French              | French            | English            |
| ge     | German              | German            | English            |
| hi     | Hindi               | Hindi             | English            |
| it     | Italian             | Italian           | English            |
| jp     | Japanese            | Japanese          | Simplified Chinese |
| jv     | Javanese            | Javanese          | English            |
| ko     | Korean              | Korean            | Korean             |
| po     | Portuguese          | Portuguese        | English            |
| pu     | Punjabi             | Punjabi           | English            |
| ru     | Russian             | Russian           | English            |
| sc     | Simplified Chinese  | Simplified Chinese| Simplified Chinese |
| sp     | Spanish             | Spanish           | English            |
| sw     | Swahili             | Swahili           | English            |
| ta     | Tamil               | Tamil             | English            |
| tc     | Traditional Chinese | Traditional Chinese| Simplified Chinese |
| tu     | Turkish             | Turkish           | English            |


## Python Project for Exploring the Structure of CoT Samples
For understanding more about how the R1 model generate the Chain-of-Thought (CoT) samples and its structure. You can find example files in the `/payload` folder, including **xx_question.txt** and **xx_answer.txt**. These files contain a sample question, **"Tell me why the sunrise is from the east"**,  translated into 20 popular languages worldwide, along with their corresponding reasoning answers.

If you'd like to experiment with generating CoT samples for your own questions or test the serverless API latency against complex queries, you can clone this project and try it out yourself. It’s a hands-on way to experience how CoT reasoning works across different languages and contexts.

### Prerequisites
- Azure Subscription
- Azure AI Foundry
- DeepSeek-R1 Serverless API deployment
- Python 3.x installed on your system
- `pip` package manager

### Step 1: Clone the Repository

First, clone the repository to your local machine:

```bash
git clone <repository_url>
cd <repository_name>
```

### Step 2: Set Up a Virtual Environment

Create a virtual environment to manage your project's dependencies:

```bash
python -m venv venv
```

Activate the virtual environment:

- On Windows:
  ```bash
  venv\Scripts\activate
  ```
- On macOS and Linux:
  ```bash
  source venv/bin/activate
  ```

### Step 3: Install Dependencies

Install the required dependencies using `pip`:

```bash
pip install -r requirements.txt
```

### Step 4: Create a Configuration File

Create a `config.yml` file in the root directory of the project with the following content:

```yaml
model:
  path: "path/to/model"
  version: "1.0"
inference:
  batch_size: 32
  device: "cuda"
api:
  key: "{your_api_key}}"  
```

### Step 5: Run the Inference Script

Execute the `inference.py` script to run the inference:

- On Windows:
  ```bash
  python inference.py -p .\payload\en_question.txt -o .\payload\en_answer.txt
  ```
- On macOS and Linux:
  ```bash
  python inference.py -p ./payload/en_question.txt -o ./payload/en_answer.txt
  ```
### Step 6: Run the Inference Script in batch mode

Execute the `run` script to run the inference for batches:

- On Windows:
  ```bash
  run.bat
  ```
- On macOS and Linux:
  ```bash
  chmod +x run.sh
  ./run.sh

  ```

### Step 7: Deactivate the Virtual Environment

Once you are done, you can deactivate the virtual environment:

```bash
deactivate
```

### Additional Notes

- Ensure that the paths and parameters in `config.yml` are correctly set according to your project's requirements.
- If you need to add more dependencies, update the `requirements.txt` file and run `pip install -r requirements.txt` again.

### References

- [DeepSeek R1 is now available on Azure AI Foundry and GitHub](https://azure.microsoft.com/en-us/blog/deepseek-r1-is-now-available-on-azure-ai-foundry-and-github/)



